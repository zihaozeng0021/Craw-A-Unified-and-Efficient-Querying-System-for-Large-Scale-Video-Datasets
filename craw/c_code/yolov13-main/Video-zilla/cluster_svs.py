import pickle
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import glob
import os
import matplotlib.pyplot as plt
import time
from sklearn.decomposition import PCA  # æå‰å¯¼å…¥PCAï¼Œé¿å…è®¡æ—¶å†…å¯¼å…¥è€—æ—¶

def omd(a, b):
    if len(a) == 0 or len(b) == 0:
        return float('inf')
    
    if np.array_equal(a, b):
        return 0.0
    
    max_features = 30
    if len(a) > max_features:
        indices = np.linspace(0, len(a)-1, max_features, dtype=int)
        a = a[indices]
    
    if len(b) > max_features:
        indices = np.linspace(0, len(b)-1, max_features, dtype=int)
        b = b[indices]
    
    a = np.array(a, dtype=np.float32) 
    b = np.array(b, dtype=np.float32)
    
    max_dim = 256
    if a.shape[1] > max_dim:
        a = a[:, :max_dim]
    if b.shape[1] > max_dim:
        b = b[:, :max_dim]
    
    min_features = min(a.shape[0], b.shape[0])
    if min_features == 0:
        return float('inf')
    
    if a.shape[0] > min_features:
        a = a[:min_features]
    if b.shape[0] > min_features:
        b = b[:min_features]
    
    try:
        dist_mat = np.zeros((len(a), len(b)), dtype=np.float32)
        for i in range(len(a)):
            for j in range(len(b)):
                dist_mat[i, j] = np.linalg.norm(a[i] - b[j])
        
        w1 = np.ones(len(a), dtype=np.float32) / len(a)
        w2 = np.ones(len(b), dtype=np.float32) / len(b)
        
        from pyemd import emd
        distance = emd(w1, w2, dist_mat)
        return float(distance)
    except Exception as e:
        try:
            avg_a = np.mean(a, axis=0)
            avg_b = np.mean(b, axis=0)
            distance = np.linalg.norm(avg_a - avg_b)
            return float(distance)
        except:
            return float('inf')

def find_optimal_clusters(distance_matrix, max_clusters=10):
    """ä½¿ç”¨è½®å»“ç³»æ•°æœ€å¤§åŒ–è‡ªåŠ¨ç¡®å®šæœ€ä¼˜èšç±»æ•°"""
    distance_matrix = np.nan_to_num(distance_matrix, nan=0.0, posinf=0.0, neginf=0.0)
    
    max_clusters = min(max_clusters, len(distance_matrix)-1)
    if max_clusters < 4:
        return 4
    
    silhouette_scores = []
    k_range = range(4, max_clusters + 1)

    print("æ­£åœ¨è®¡ç®—ä¸åŒèšç±»æ•°çš„è½®å»“ç³»æ•°...")
    
    for k in k_range:
        try:
            clustering = AgglomerativeClustering(
                n_clusters=k,
                metric='precomputed',
                linkage='average'
            )
            labels = clustering.fit_predict(distance_matrix)
            
            if len(np.unique(labels)) > 1:
                silhouette_avg = silhouette_score(distance_matrix, labels, metric='precomputed')
                silhouette_scores.append(silhouette_avg)
                print(f"  k={k}: è½®å»“ç³»æ•°={silhouette_avg:.4f}")
            else:
                silhouette_scores.append(-1)
                print(f"  k={k}: è½®å»“ç³»æ•°=-1.0000 (æ— æ•ˆ)")
        except Exception as e:
            print(f"  k={k}: è®¡ç®—å‡ºé”™ - {e}")
            silhouette_scores.append(-1)
    
    if silhouette_scores:
        optimal_k = k_range[np.argmax(silhouette_scores)]
        max_silhouette = max(silhouette_scores)
        print(f"æœ€ä¼˜èšç±»æ•°: {optimal_k} (è½®å»“ç³»æ•°: {max_silhouette:.4f})")
        return optimal_k
    else:
        print("æ— æ³•ç¡®å®šæœ€ä¼˜èšç±»æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼4")
        return 4

def compute_omd_distance_matrix(svs_features_list):
    """è®¡ç®—æ‰€æœ‰SVSä¹‹é—´çš„OMDè·ç¦»çŸ©é˜µ"""
    n = len(svs_features_list)
    distance_matrix = np.zeros((n, n))
    
    print("è®¡ç®—OMDè·ç¦»çŸ©é˜µ...")
    for i in range(n):
        for j in range(i+1, n):
            distance = omd(svs_features_list[i], svs_features_list[j])
            distance_matrix[i, j] = distance
            distance_matrix[j, i] = distance
        if (i+1) % 10 == 0 or i+1 == n:
            print(f"  è¿›åº¦: {i+1}/{n}")
    
    return distance_matrix

def cluster_svs(svs_features_list, num_clusters=None):
    """ä»…æ‰§è¡Œæ ¸å¿ƒèšç±»é€»è¾‘ï¼ˆè¾“å…¥å·²åŠ è½½çš„ç‰¹å¾ï¼Œé¿å…è®¡æ—¶å†…è¯»æ–‡ä»¶ï¼‰"""
    # ---------------------- æ ¸å¿ƒèšç±»è®¡æ—¶å¼€å§‹ ----------------------
    cluster_start_time = time.perf_counter()
    
    # 1. è®¡ç®—OMDè·ç¦»çŸ©é˜µï¼ˆæ ¸å¿ƒæ­¥éª¤1ï¼‰
    distance_matrix = compute_omd_distance_matrix(svs_features_list)
    
    # 2. ç¡®å®šèšç±»æ•°é‡ï¼ˆæ ¸å¿ƒæ­¥éª¤2ï¼‰
    if num_clusters is None:
        actual_num_clusters = find_optimal_clusters(distance_matrix)
    else:
        actual_num_clusters = min(num_clusters, len(svs_features_list))
    
    print(f"æ‰§è¡Œèšç±»åˆ†æï¼Œèšç±»æ•°é‡: {actual_num_clusters}")
    
    # 3. æ‰§è¡Œå±‚æ¬¡èšç±»ï¼ˆæ ¸å¿ƒæ­¥éª¤3ï¼‰
    try:
        clustering = AgglomerativeClustering(
            n_clusters=actual_num_clusters,
            metric='precomputed',
            linkage='average'
        )
    except TypeError:
        clustering = AgglomerativeClustering(
            n_clusters=actual_num_clusters,
            affinity='precomputed',
            linkage='average'
        )
    labels = clustering.fit_predict(distance_matrix)
    

    cluster_end_time = time.perf_counter()
    core_cluster_duration = cluster_end_time - cluster_start_time
    print(f"\nã€æ ¸å¿ƒèšç±»æ€»è€—æ—¶ã€‘: {core_cluster_duration:.2f} ç§’") 
    
    return labels, distance_matrix, actual_num_clusters, core_cluster_duration

def visualize_clusters(svs_features_list, labels, save_path):
    """
    ä¼˜åŒ–çš„èšç±»å¯è§†åŒ–å‡½æ•°ï¼š
    1. ç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„ç‰¹å¾åˆ—è¡¨ï¼Œé¿å…é‡å¤è¯»æ–‡ä»¶
    2. æ£€æŸ¥ä¿å­˜è·¯å¾„ç›®å½•ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
    3. å¢å¼ºå¼‚å¸¸æ•è·ï¼Œè¾“å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
    """
    try:
        # 1. æ£€æŸ¥ä¿å­˜è·¯å¾„çš„ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        save_dir = os.path.dirname(save_path)
        if not os.path.exists(save_dir):
            os.makedirs(save_dir, exist_ok=True)
            print(f"å·²åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•: {save_dir}")
        
        # 2. è®¡ç®—æ¯ä¸ªSVSçš„å¹³å‡ç‰¹å¾ï¼ˆç”¨äºPCAé™ç»´ï¼‰
        all_avg_features = []
        for features in svs_features_list:
            avg_feature = np.mean(features, axis=0)  # å•SVSçš„å¹³å‡ç‰¹å¾
            all_avg_features.append(avg_feature)
        all_avg_features = np.array(all_avg_features)
        
        # 3. PCAé™ç»´åˆ°2Dï¼ˆä¾¿äºå¯è§†åŒ–ï¼‰
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(all_avg_features)  # æ‰€æœ‰SVSçš„2Dç‰¹å¾
        
        # 4. è®¡ç®—æ¯ä¸ªèšç±»çš„ä¸­å¿ƒï¼ˆ2Dç©ºé—´ï¼‰
        unique_labels = np.unique(labels)
        cluster_centers_2d = []
        for label in unique_labels:
            # å–è¯¥èšç±»æ‰€æœ‰SVSçš„2Dç‰¹å¾ï¼Œè®¡ç®—ä¸­å¿ƒ
            cluster_2d_features = features_2d[labels == label]
            cluster_center_2d = np.mean(cluster_2d_features, axis=0)
            cluster_centers_2d.append(cluster_center_2d)
        cluster_centers_2d = np.array(cluster_centers_2d)
        
        # 5. ç»˜åˆ¶å¯è§†åŒ–å›¾
        plt.figure(figsize=(12, 8))  # æ”¾å¤§å›¾å°ºå¯¸ï¼Œæå‡æ¸…æ™°åº¦
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', 
                  '#FF9FF3', '#54A0FF', '#5F27CD', '#00D2D3', '#FF9F43']  # æ›´é²œæ˜çš„é…è‰²
        
        # ç»˜åˆ¶æ¯ä¸ªèšç±»çš„SVSç‚¹
        for idx, label in enumerate(unique_labels):
            cluster_points = features_2d[labels == label]
            plt.scatter(
                cluster_points[:, 0], cluster_points[:, 1],
                c=colors[idx % len(colors)],  # å¾ªç¯ä½¿ç”¨é¢œè‰²
                label=f'Cluster {label} (n={len(cluster_points)})',  # æ ‡æ³¨èšç±»åŒ…å«çš„SVSæ•°é‡
                alpha=0.8,  # æå‡ç‚¹çš„é€æ˜åº¦ï¼Œé¿å…é‡å é®æŒ¡
                s=80  # æ”¾å¤§ç‚¹çš„å°ºå¯¸ï¼Œæ›´æ˜“è§‚å¯Ÿ
            )
        
        # ç»˜åˆ¶èšç±»ä¸­å¿ƒï¼ˆé»‘è‰²å‰å·ï¼Œçªå‡ºæ˜¾ç¤ºï¼‰
        plt.scatter(
            cluster_centers_2d[:, 0], cluster_centers_2d[:, 1],
            c='black', marker='x', s=300, linewidths=4,
            label='Cluster Centers', zorder=5  # zorderç¡®ä¿ä¸­å¿ƒåœ¨æœ€ä¸Šå±‚
        )
        
        # å›¾æ³¨ä¸æ ¼å¼ä¼˜åŒ–
        plt.title('SVS Clustering Result Visualization (PCA 2D)', fontsize=16, fontweight='bold')
        plt.xlabel('PCA Component 1', fontsize=12)
        plt.ylabel('PCA Component 2', fontsize=12)
        plt.legend(fontsize=10, loc='best')  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å›¾ä¾‹ä½ç½®
        plt.grid(True, alpha=0.3, linestyle='--')  # è™šçº¿ç½‘æ ¼ï¼Œæå‡å¯è¯»æ€§
        plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé¿å…å…ƒç´ è¢«æˆªæ–­
        
        # ä¿å­˜å›¾ç‰‡ï¼ˆä½¿ç”¨é«˜åˆ†è¾¨ç‡ï¼‰
        plt.savefig(
            save_path,
            dpi=300,  # é«˜åˆ†è¾¨ç‡ï¼Œé€‚åˆåç»­åˆ†æ
            bbox_inches='tight'  # é¿å…å›¾ä¾‹è¢«æˆªæ–­
        )
        plt.close()  # å…³é—­å›¾ï¼Œé‡Šæ”¾å†…å­˜
        print(f"\nâœ… èšç±»å¯è§†åŒ–å›¾å·²æˆåŠŸä¿å­˜åˆ°: {save_path}")

    except Exception as e:
        # è¾“å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜
        print(f"\nâŒ èšç±»å¯è§†åŒ–å¤±è´¥:")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"  é”™è¯¯è¯¦æƒ…: {str(e)}")

if __name__ == "__main__":
    # ---------------------- 1. åŠ è½½SVSç‰¹å¾æ–‡ä»¶ ----------------------
    file_load_start = time.perf_counter()
    
    svs_dir = '/home/nanchang/ZZQ/yolov13/main/yolov13-main/Video-zilla/warsaw/origin'
    svs_files = sorted(glob.glob(f"{svs_dir}/*.pkl"))
    svs_features_list = []
    valid_svs_files = []
    
    if not svs_files:
        exit()
    
    for svs_file in svs_files:
        try:
            with open(svs_file, 'rb') as f:
                features = pickle.load(f)
                if len(features) > 0 and features.ndim == 2:  # é¢å¤–æ£€æŸ¥ç‰¹å¾æ ¼å¼ï¼ˆç¡®ä¿æ˜¯2Dæ•°ç»„ï¼‰
                    svs_features_list.append(features)
                    valid_svs_files.append(svs_file)
                else:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶ {os.path.basename(svs_file)}: ç‰¹å¾ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶ {os.path.basename(svs_file)} æ—¶å‡ºé”™: {str(e)}")
    
    if len(svs_features_list) == 0:
        print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆSVSç‰¹å¾æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        exit()
    
    file_load_end = time.perf_counter()
    print(f"\nğŸ“¥ æ–‡ä»¶åŠ è½½å®Œæˆ:")
    print(f"  - æœ‰æ•ˆSVSæ•°é‡: {len(svs_features_list)}")
    print(f"  - åŠ è½½è€—æ—¶: {file_load_end - file_load_start:.2f} ç§’")

    # ---------------------- 2. æ‰§è¡Œæ ¸å¿ƒèšç±» ----------------------
    labels, distance_matrix, actual_num_clusters, core_cluster_time = cluster_svs(svs_features_list)

    # ---------------------- 3. é€‰æ‹©èšç±»ä»£è¡¨ç‚¹ & ä¿å­˜èšç±»ç»“æœ ----------------------
    result_process_start = time.perf_counter()
    
    # é€‰æ‹©æ¯ä¸ªèšç±»çš„"ä¸­å¿ƒä»£è¡¨"ï¼ˆå¹³å‡è·ç¦»æœ€å°çš„SVSï¼‰
    cluster_representatives = []
    for i in range(actual_num_clusters):
        cluster_indices = np.where(labels == i)[0]
        min_avg_distance = float('inf')
        representative_idx = cluster_indices[0]
        
        for idx in cluster_indices:
            avg_distance = np.mean(distance_matrix[idx][cluster_indices])
            if avg_distance < min_avg_distance:
                min_avg_distance = avg_distance
                representative_idx = idx
        
        representative_file = valid_svs_files[representative_idx]
        cluster_representatives.append(representative_file)
        print(f"\nğŸ·ï¸  èšç±» {i}:")
        print(f"  - ä»£è¡¨SVS: {os.path.basename(representative_file)}")
        print(f"  - åŒ…å«SVSæ•°é‡: {len(cluster_indices)}")
    
    # ä¿å­˜èšç±»ç»“æœï¼ˆåŒ…å«å¯è§†åŒ–æ‰€éœ€ä¿¡æ¯ï¼‰
    cluster_result_path = '/home/nanchang/ZZQ/yolov13/main/yolov13-main/Video-zilla/warsaw/origin/cluster_result.pkl'
    cluster_info = {
        'labels': labels,
        'svs_files': valid_svs_files,
        'representatives': cluster_representatives,
        'core_cluster_time_seconds': core_cluster_time,
        'optimal_clusters': actual_num_clusters  # è¡¥å……æœ€ä¼˜èšç±»æ•°
    }
    with open(cluster_result_path, 'wb') as f:
        pickle.dump(cluster_info, f)
    print(f"\nğŸ’¾ èšç±»ç»“æœå·²ä¿å­˜åˆ°: {cluster_result_path}")
    
    result_process_end = time.perf_counter()


    cluster_img_path = '/home/nanchang/ZZQ/yolov13/main/yolov13-main/Video-zilla/warsaw/origin/cluster_visualization.png'
    visualize_clusters(
        svs_features_list=svs_features_list,  # ç›´æ¥ä¼ å·²åŠ è½½çš„ç‰¹å¾ï¼Œé¿å…é‡å¤è¯»æ–‡ä»¶
        labels=labels,
        save_path=cluster_img_path
    )

    # ---------------------- 5. è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ ----------------------
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - æ ¸å¿ƒèšç±»è€—æ—¶: {core_cluster_time:.2f} ç§’ï¼ˆè·ç¦»çŸ©é˜µè®¡ç®—+èšç±»åˆ†æï¼‰")
    print(f"  - ç»“æœå¤„ç†è€—æ—¶: {result_process_end - result_process_start:.2f} ç§’ï¼ˆä»£è¡¨ç‚¹é€‰æ‹©+ç»“æœä¿å­˜ï¼‰")
    print(f"  - æ€»è€—æ—¶: {time.perf_counter() - file_load_start:.2f} ç§’")
    print(f"  - å¯è§†åŒ–å›¾è·¯å¾„: {cluster_img_path}")